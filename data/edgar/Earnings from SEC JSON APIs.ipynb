{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c3e02c4-ab09-41c3-b7c6-caeb361c767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 07:57:04,516 - INFO - Loading cached file for https://www.sec.gov/include/ticker.txt\n",
      "2024-09-21 07:57:04,519 - INFO - Loading cached file for https://data.sec.gov/api/xbrl/companyconcept/CIK0000320193/us-gaap/EarningsPerShareBasic.json\n",
      "2024-09-21 07:57:04,521 - INFO - Loading cached file for https://data.sec.gov/api/xbrl/companyconcept/CIK0000320193/us-gaap/EarningsPerShareDiluted.json\n",
      "2024-09-21 07:57:04,522 - INFO - Downloading https://data.sec.gov/api/xbrl/companyconcept/CIK0000320193/us-gaap/NetIncomeLoss.json\n",
      "2024-09-21 07:57:05,730 - INFO - Downloading https://data.sec.gov/api/xbrl/companyconcept/CIK0000320193/us-gaap/OperatingIncomeLoss.json\n",
      "2024-09-21 07:57:06,762 - INFO - Downloading https://data.sec.gov/api/xbrl/companyconcept/CIK0000320193/us-gaap/GrossProfit.json\n",
      "2024-09-21 07:57:07,988 - INFO - Downloading https://data.sec.gov/api/xbrl/companyconcept/CIK0000320193/us-gaap/Revenues.json\n",
      "2024-09-21 07:57:09,064 - INFO - Loading cached file for https://www.sec.gov/include/ticker.txt\n",
      "2024-09-21 07:57:09,071 - INFO - Saved financial data to fundamentals/aapl-0000320193-EPS.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Financial data saved to fundamentals/aapl-0000320193-EPS.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import pandas as pd  # For DataFrame and CSV handling\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Headers for SEC API requests\n",
    "API_HEADERS = {\n",
    "    'User-Agent': 'Your Name yourname@example.com',  # Replace with your information\n",
    "    'Accept': 'application/json',\n",
    "}\n",
    "\n",
    "HTML_HEADERS = {\n",
    "    'User-Agent': 'Your Name yourname@example.com',  # Replace with your information\n",
    "    'Accept-Encoding': 'gzip, deflate',\n",
    "    'Host': 'www.sec.gov'\n",
    "}\n",
    "\n",
    "def ensure_directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def get_cached_filename(url, directory):\n",
    "    return os.path.join(directory, hashlib.md5(url.encode()).hexdigest() + '.json')\n",
    "\n",
    "def download_with_cache(url, cache_dir='sec_cache', headers=None):\n",
    "    ensure_directory_exists(cache_dir)\n",
    "    cached_file = get_cached_filename(url, cache_dir)\n",
    "    \n",
    "    if os.path.exists(cached_file):\n",
    "        logging.info(f\"Loading cached file for {url}\")\n",
    "        with open(cached_file, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    \n",
    "    logging.info(f\"Downloading {url}\")\n",
    "    try:\n",
    "        if headers is None:\n",
    "            headers = API_HEADERS\n",
    "        session = requests.Session()\n",
    "        response = session.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        content = response.text\n",
    "        \n",
    "        with open(cached_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(content)\n",
    "        \n",
    "        time.sleep(0.2)  # Respectful delay after a new download\n",
    "        return content\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error downloading {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_cik(ticker):\n",
    "    ticker = ticker.upper()\n",
    "    url = \"https://www.sec.gov/include/ticker.txt\"\n",
    "    content = download_with_cache(url, 'sec_data')\n",
    "    if content:\n",
    "        for line in content.splitlines():\n",
    "            t, c = line.strip().split('\\t')\n",
    "            if t.upper() == ticker:\n",
    "                return c.zfill(10)\n",
    "    logging.error(f\"CIK not found for ticker {ticker}\")\n",
    "    return None\n",
    "\n",
    "def get_financial_data(ticker, concepts):\n",
    "    cik = get_cik(ticker)\n",
    "    if not cik:\n",
    "        logging.error(f\"Failed to get CIK for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    financial_data = {}\n",
    "    for concept in concepts:\n",
    "        url = f\"https://data.sec.gov/api/xbrl/companyconcept/CIK{cik}/us-gaap/{concept}.json\"\n",
    "        content = download_with_cache(url, cache_dir='sec_data', headers=API_HEADERS)\n",
    "        if not content:\n",
    "            logging.error(f\"Failed to get data for {ticker} and concept {concept}\")\n",
    "            continue\n",
    "        data = json.loads(content)\n",
    "        concept_data = []\n",
    "        for unit in data.get('units', {}):\n",
    "            for fact in data['units'][unit]:\n",
    "                concept_data.append({\n",
    "                    'date': fact.get('end'),\n",
    "                    'value': fact.get('val'),\n",
    "                    'unit': unit,\n",
    "                    'concept': concept,\n",
    "                    'form': fact.get('form'),\n",
    "                    'filed': fact.get('filed'),\n",
    "                    'fy': fact.get('fy'),\n",
    "                    'fp': fact.get('fp'),\n",
    "                })\n",
    "        # Sort the list by date in descending order\n",
    "        concept_data.sort(key=lambda x: x['date'], reverse=True)\n",
    "        financial_data[concept] = concept_data\n",
    "    return financial_data\n",
    "\n",
    "def save_financial_data_to_csv(ticker, financial_data, concepts):\n",
    "    cik = get_cik(ticker)\n",
    "    if not cik:\n",
    "        logging.error(f\"Failed to get CIK for {ticker}\")\n",
    "        return None\n",
    "    \n",
    "    # Create 'fundamentals' folder if it doesn't exist\n",
    "    ensure_directory_exists('fundamentals')\n",
    "    \n",
    "    # Collect all dates\n",
    "    all_dates = set()\n",
    "    for concept in financial_data:\n",
    "        for item in financial_data[concept]:\n",
    "            all_dates.add(item['date'])\n",
    "    all_dates = sorted(all_dates, reverse=True)\n",
    "    \n",
    "    # Build rows\n",
    "    rows = []\n",
    "    for date in all_dates:\n",
    "        row = {'date': date}\n",
    "        for concept in concepts:\n",
    "            # Find the value for this date\n",
    "            value = None\n",
    "            unit = None\n",
    "            form = None\n",
    "            filed = None\n",
    "            fy = None\n",
    "            fp = None\n",
    "            for item in financial_data.get(concept, []):\n",
    "                if item['date'] == date:\n",
    "                    value = item['value']\n",
    "                    unit = item['unit']\n",
    "                    form = item['form']\n",
    "                    filed = item['filed']\n",
    "                    fy = item['fy']\n",
    "                    fp = item['fp']\n",
    "                    break\n",
    "            row[concept] = value\n",
    "            # Include unit for each concept\n",
    "            row[f'{concept}_unit'] = unit\n",
    "        # Include additional metadata (only if available)\n",
    "        row['form'] = form\n",
    "        row['filed'] = filed\n",
    "        row['fy'] = fy\n",
    "        row['fp'] = fp\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Save to CSV\n",
    "    filename = f\"fundamentals/{ticker.lower()}-{cik}-EPS.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    logging.info(f\"Saved financial data to {filename}\")\n",
    "    return filename\n",
    "\n",
    "# Main code\n",
    "ticker = \"AAPL\"  # Replace with your desired ticker\n",
    "\n",
    "# Define the financial concepts to fetch\n",
    "concepts = [\n",
    "    'EarningsPerShareBasic',\n",
    "    'EarningsPerShareDiluted',\n",
    "    'NetIncomeLoss',\n",
    "    'OperatingIncomeLoss',\n",
    "    'GrossProfit',\n",
    "    'Revenues'  # Also known as SalesRevenueNet for some companies\n",
    "]\n",
    "\n",
    "financial_data = get_financial_data(ticker, concepts)\n",
    "\n",
    "if financial_data:\n",
    "    filename = save_financial_data_to_csv(ticker, financial_data, concepts)\n",
    "    print(f\"Financial data saved to {filename}\")\n",
    "else:\n",
    "    print(\"Failed to retrieve financial data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa0e60-fa99-4789-b0ab-6e2fddca8aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
